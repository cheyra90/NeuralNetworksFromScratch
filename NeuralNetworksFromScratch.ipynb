{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Neurons\n",
    "\n",
    "Within **Neural Networks**, the concept of a neuron is based on the biological neuron cell which has dendrites on one end which recieve input and an axon terminus which fires an output response and communicates that response with downstream cells.\n",
    "\n",
    "<img src='assets/Neuron 1.PNG'>\n",
    "\n",
    "Neural networks have revolutionised modern machine learning and allow for complex relationships in large datasets to be discovered and modelled. In machine learning, a Neuron has 3 distinct parts:\n",
    "\n",
    "- Inputs\n",
    "-- The inputs are the values which are fed into the Neuron\n",
    "\n",
    "- Weights\n",
    "-- The weights are a representation of the computation that the neuron does on the input values, these are the values which are trained when the machine learns.\n",
    "\n",
    "- Biases\n",
    "-- Biases act as a verticle scaling value, these modify the degree to which certain neurons and weights are considered. These are also modified during machine learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# A simple neuron can be coded in python like this:\n",
    "\n",
    "inputs = [1, 2, 3, 4]\n",
    "\n",
    "weights = [0.5, 0.8, -0.2, 0.85]\n",
    "\n",
    "biases = 2\n",
    "\n",
    "# the values are then calculated forward to generate the output value from the first neuron.\n",
    "\n",
    "output = (inputs[0] * weights[0] +\n",
    "        inputs[1] * weights[1] + \n",
    "        inputs[2] * weights[2] + \n",
    "        inputs[3] * weights[3] + biases)\n",
    "\n",
    "output"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.9"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[6.9, 5.96, 1.05]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# to add more neurons we can expand the above code\n",
    "\n",
    "inputs = [1, 2, 3, 4]\n",
    "\n",
    "weights1 = [0.5, 0.8, -0.2, 0.85]\n",
    "weights2 = [0.22, 0.14, 0.82, 0.25]\n",
    "weights3 = [-0.5, 0.27, -0.17, 0.23]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 2\n",
    "bias3 = 0.6\n",
    "\n",
    "outputs = [\n",
    "    inputs[0] * weights1[0] + \n",
    "    inputs[1] * weights1[1] + \n",
    "    inputs[2] * weights1[2] + \n",
    "    inputs[3] * weights1[3] + bias1,\n",
    "\n",
    "    inputs[0] * weights2[0] + \n",
    "    inputs[1] * weights2[1] + \n",
    "    inputs[2] * weights2[2] + \n",
    "    inputs[3] * weights2[3] + bias2,\n",
    "\n",
    "    inputs[0] * weights3[0] + \n",
    "    inputs[1] * weights3[1] + \n",
    "    inputs[2] * weights3[2] + \n",
    "    inputs[3] * weights3[3] + bias3,\n",
    "]\n",
    "\n",
    "outputs\n",
    "\n",
    "# we are now presented with 3 outputs, one for each of our neurons. This is what is known as a fully connected layer, although this is common, it is not a requirement that all inputs are connected to all neurons. The concept of Dropout which prevents overfitting is a method by which a random neuron will be turned off for a training cycle and will not be used to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[6.9, 5.96, 1.05]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# There is a much better way to write this calculation. \n",
    "\n",
    "inputs = [1, 2, 3, 4]\n",
    "\n",
    "weights = [\n",
    "            [0.5, 0.8, -0.2, 0.85],\n",
    "            [0.22, 0.14, 0.82, 0.25],\n",
    "            [-0.5, 0.27, -0.17, 0.23]\n",
    "            ]\n",
    "\n",
    "biases = [2, 2, 0.6]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "layer_outputs"
   ]
  },
  {
   "source": [
    "## Dot Product\n",
    "\n",
    "In the above examples, the calculation which was taking place is something known as the **dot product**. This means that we take the (n)th element from one array and multiply it with the corresponding element from another array.\n",
    "\n",
    "This can be represented mathematically as:\n",
    "\n",
    "<img src='assets/Dot Product.PNG'>\n",
    "\n",
    "There is a function in numpy which will do this for us called *<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.dot.html\">np.dot</a>* we can now re-write the above much more simply with numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6.9 , 5.96, 1.05])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 4] # we could also turn these into numpy arrays with np.array()\n",
    "\n",
    "weights = [\n",
    "            [0.5, 0.8, -0.2, 0.85],\n",
    "            [0.22, 0.14, 0.82, 0.25],\n",
    "            [-0.5, 0.27, -0.17, 0.23]\n",
    "            ]\n",
    "\n",
    "biases = [2, 2, 0.6]\n",
    "\n",
    "outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "outputs"
   ]
  },
  {
   "source": [
    "## Batches\n",
    "\n",
    "Practically when training neural networks, input data is divided up into batches and fed into the network, this is done to reduce memory overhead, as it allows for parallelisation and also because if we trained the forward and backward pass operations for each datapoint one at a time, the network would not generalise well and would overfit to each datapoint. \n",
    "\n",
    "The size of batches becomes a common **hyperparameter** which is tuned when **Neural Networks** are created.\n",
    "\n",
    "## Matrix Product\n",
    "\n",
    "This presents a problem though, we want to be able to carry out the calculations on the batches of data quickly in order to do this we use a matrix product, this is essentially a dot product against all combinations of the rows from the first (input) matrix and the columns from the second (weights) matrix, this results in an output matrix which has holds the values for these computations. The second (weights) matrix is also transposed to allow for this to work, you can transpose any numpy array by using <a hred=\"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html\">np.T</a>\n",
    "\n",
    "<img src='assets/Matrix Product.PNG'>\n",
    "\n",
    "there is also a useful visualisation of this process <a href=\"https://nnfs.io/jei\">here</a>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[6.9  , 5.96 , 1.05 ],\n",
       "       [9.715, 3.587, 1.731],\n",
       "       [2.07 , 4.554, 1.334]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "inputs = [\n",
    "        [1, 2, 3, 4],\n",
    "         [2, 5, -0.4, 3.1],\n",
    "         [-1.5, 2.7, 3.3, -0.8]\n",
    "         ]\n",
    "\n",
    "weights  = [\n",
    "            [0.5, 0.8, -0.2, 0.85],\n",
    "            [0.22, 0.14, 0.82, 0.25],\n",
    "            [-0.5, 0.27, -0.17, 0.23]\n",
    "            ]\n",
    "\n",
    "biases =  [2, 2, 0.6]\n",
    "\n",
    "outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "outputs"
   ]
  },
  {
   "source": [
    "## Layers\n",
    "\n",
    "Our current network functions but it currently only has one layer, in order to model complex relationships usually more layers are added, a neural network can be categorised as **deep** when it has more than two layers of neurons. Layers will often be referred to as **Hidden Layers**, this does not mean that you cannot see what is going on here but it is more accurate that as networks grow, it becomes impractical for people to understand how the weights and biases of a given neuron represent the model in the real world."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}